--------------------------------------------
	编译原理大作业：汉语词法分析器
		作者：成立
	      日期：2010/4/3
--------------------------------------------
1.简介
    本程序的功能是将由西文字符、扩展字符、特殊字符组成的文件流切分为记号流(Token stream），并且标注上可能的词性（或者类别，如名词，动词等）。

2.前言
    汉语真是一门博大精深的语言，要制作出对汉语非常完美的词法分析器，几乎是不可能的，这点也早在编写程序之前就已经预料到了。尽管我总是希望将其做的更接近完美，但是仔细思考了下汉语词法以后，就感觉我面对的是茫茫大海，实在是举足无措。也难怪，汉语词法分析器都可以作为硕士生的论文了，这都上升到语言科学的高度了，想来也不是我这种语文白痴能搞定的。
    而从汉语本身来说，她是以歧义出名的，词法上的歧义已经多得出奇了，到语法层面上就更不堪想象了。比如简单的句子：“我对面的人很漂亮” 和“我对面食很感兴趣”，对于这两个句子，如何拆分“对”和“面”这两个字？再比如我的名字“成立”，它到底是我的名字呢，还是一个动词呢？往往对于大脑来说理解起来轻而易举的事情，计算机就必须要通过非常复杂的算法来完成。
    综上说述，以及考虑到个人的知识面、编程能力、精力时间，我只能做出一个非常基础的汉语词法分析器。为了验证其有效性，我特地从网上down了一篇政府发布的文章作为测试样例，因为一般这种文章用词都非常的规范和正式，识别的难度相对会降低很多，而且实际的效果也确实很不错，至少这篇文章是能够100%识别出来的。
    我的这个词法分析器，主要的组成部件有三：字典、策略机制和词法分析器。
    第一部分，字典。对于像汉语这样庞大的语言，字典是必须具备的，并且它是影响性能非常关键的一个因素。在本程序中，我使用了trie结构来实现字典。
    第二部分，策略机制。非常有限地解决一部分歧义问题。
    第三部分，词法分析器。这个就是本程序的核心部件，它是整个分析过程的“司令”，并且适时地调用上面两个部件共同完成工作。
    最后，还有一个实用函数库，用于解决ansi字符流中汉字转换与输出的相关问题。
    总之，如果你抱着对待白痴的心态来玩这个分析器，那么它也许还有些娱乐价值和潜力可挖掘。但是如果你抱着吹毛求疵的心态来“折磨”它，那么对不起，它会让你失望透顶的……谢谢！

3.源代码组织
    所有源代码均位于src目录之下。下面是各文件的简介：
    chs.c	包含main函数的主程序文件
    global.h	一些全局宏、参数、变量的声明
    lex.c	词法分析器的实现
    lex.h	词法分析器的接口
    tokens.c	记号类别的详细信息
    tokens.h	记号的结构（或者叫元数据）
    rule.c	策略机制的实现
    rule.h	策略机制的接口
    trie.c	trie树的实现
    trie.h	trie树的接口
    util.c	实用函数的实现
    util.h	实用函数的接口

4.编译及使用方法
    我已经为本程序写了一个非常简单的makefile，供gnu make调用。也就是说，它只能用gcc来编译，用m$vc的请绕道，或者直接运行已经编译好的程序。
    要编译本程序，只需要简单的执行项目根目录下的build批处理文件，它会自动启动编译过程。编译之后的exe文件在bin文件夹下可以找到。
    编译过程中应该是不会出现任何错误或警告的。现在进入到bin目录中，有两个批处理文件可以给你运行，都是对sample.txt这篇文章的词法分析调用。区别在于，可读模式是用更易于人读懂的格式显示出来。
    你也可以自己调用chs.exe来载入自己的文章。chs.exe的使用方法：
    (1) chs -ctf
        当你编辑好自己的词典以后，必须使用这个命令，重新建立trie文件，才会被程序用于词法分析。trie文件是什么？待会儿会讲的。
    (2) chs [-h] <filename>
        filename是必须包含的，输入文件名。
        -h选项是可选的，即指定程序以更易读的方式显示解析出的记号流。否则……你自己试试看吧。

5.关键设计详解
5.1 程序所涉及的文件
5.1.1 字典文件
    位于bin目录，或者说应用程序目录中的data子目录下。文件名固定为dict，没有后缀名。可以用记事本直接打开编辑，详细的格式在该文件最前面的注释中已经说明了。

5.1.2 trie文件
    同样位于data子目录下，文件名固定为trie，没有后缀名。该文件存储为二进制格式，是源代码中TrieNode结构的序列化，所以你不用自己去阅读，更不要去hack，否则后果自负。这个文件是本程序根据你写好的字典文件生成的，在逻辑上是等价的，只是使用了trie技术使得查字典的速度更快。所以当你修改了字典文件以后，一定要使用chs -ctf命令重建trie文件，才能在分析过程中生效。trie技术会在后文详述。

5.2 trie技术
    前面说了，本程序必须使用字典，并且必须要很高效地使用。如果我们在查字典时，直接在字典文件里面搜索，那么为词法分析（字典文件的词法分析）、I\O操作（要读文件吧？）等操作要花费非常大的开销，并且顺序查找的时间复杂度为O(n)，当词条数量很大以后，这个速度是不可以忍受的。所以我使用了trie技术，简言之，它就是把一个词语拆成单独的字，进行共享索引的方法。比如“我”和“我们”就可以共享一个含有“我”字的结点，再比如“数据库”和“数据挖掘”这两个词语，就可以共享分别含有“数”和“据”的两个结点。所以trie技术本质上也是一种查找树的结构，其查找复杂度只和所查找词语的长度成正比。
    在本程序中，这颗树的结点用TrieNode结构来存储。进一步，如果每次在程序启动时，读取字典文件，然后生成trie树，还是太浪费时间，所以我想到可以把trie树按前序遍历顺序持久化到trie文件中。这样只要在下次启动时，直接从文件中一次性复制整个trie树的“内存映像”到内存中，并修正其中的指针，就可以直接使用trie树了。
    这样做效率就高多了。我们只需要在用户编辑了字典文件后，一次性重建trie文件。就可以一直使用下去，而不用再次花时间去解析相同的字典文件。

5.3 策略机制
    目的只有一个：在有限范围内，解决一部分的歧义问题。
    我目前仍然没有想出一个比较系统的策略机制的设计，所以该模块现在等于仅有一个粗略的框架而已，只是简单展示下使用策略大概是什么意思。

6.一些问题说明
    首先是词性判别。词性判别可以说是汉语词法分析的附属品，因为汉字的词语不是简单能依靠固定的字符分隔的，分隔词语有时不得不依赖于语法，甚至语义。即使正确确定了词语的边界，有时词性也很难界定，所以这里本程序给出的词性只能作为参考！
    然后是本项目中用的wchar类型。该类型实际为unsigned short类型，占用16个位，可以保存不同字符集中的字符。但是千万注意：该类型不是Unicode类型，不是wchar_t类型，甚至不能与它们通用！它仅仅在本项目中起作用。与wchar类型相关的转换，输出功能均在util模块中实现。
    最后……请您手下留情，不要随便拿一句话就去试，然后很爽地看到它分析错了。请首先把你话中所有的词语都写到字典里去（如果还没有的话），否则不要指望它能正确分隔出来。